{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime as dt\n",
    "import folium\n",
    "from folium import plugins\n",
    "import geopandas\n",
    "from calendar import monthrange\n",
    "\n",
    "spray_data = pd.read_csv(\n",
    "    'Files/spray.csv',  # шлях до файлу, який містить дані\n",
    "    sep=',',  # роздільник стовпців\n",
    "    header=0  # номер рядка, що містить заголовок (якщо заголовка немає, можна передати None)\n",
    ")\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    'Files/train.csv',  # шлях до файлу, який містить дані\n",
    "    sep=',',  # роздільник стовпців\n",
    "    \n",
    "    header=0  # номер рядка, що містить заголовок (якщо заголовка немає, можна передати None)\n",
    ")\n",
    "\n",
    "weather_data = pd.read_csv(\n",
    "    'Files/weather.csv',  # шлях до файлу, який містить дані\n",
    "    sep=',',  # роздільник стовпців\n",
    "    header=0  # номер рядка, що містить заголовок (якщо заголовка немає, можна передати None)\n",
    ")\n",
    "\n",
    "test_truncated_data = pd.read_excel(\n",
    "    'Files/test_truncated.xlsx',\n",
    "    sheet_name='test_truncated',  # прочитати всі листи\n",
    "    usecols=\"A:J\"\n",
    "    #nrows=5\n",
    ")\n",
    "#test_truncated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime(train_data):\n",
    "    train_data['day']=pd.to_datetime(train_data.Date).dt.day\n",
    "    train_data['weekday']=pd.to_datetime(train_data.Date).dt.weekday\n",
    "    train_data['month']=pd.to_datetime(train_data.Date).dt.month\n",
    "    train_data['year']=pd.to_datetime(train_data.Date).dt.year\n",
    "    return train_data\n",
    "\n",
    "def bar_chat(_data, grop_list_, agr_colum, agr_func):\n",
    "    grop_list=[grop_list_]\n",
    "    agg_func_math = {\n",
    "        agr_colum: [agr_func]\n",
    "    }\n",
    "    plot_mas = _data.groupby(grop_list, group_keys=True, dropna=True).agg(agg_func_math).to_dict()[(agr_colum, agr_func)] \n",
    "    fig = plt.figure()\n",
    "    plt.bar(plot_mas.keys(), plot_mas.values())\n",
    "    plt.title(f'{grop_list_} {agr_colum} {agr_func} chart')    \n",
    "    plt.grid(True)   # лінії допоміжної сітки\n",
    "#bar_chat(test_truncated_data, 'year', 'year', 'count')\n",
    "\n",
    "def group_data(_data, grop_list_, agr_colum, agr_func):\n",
    "    grop_list=[grop_list_]\n",
    "    agg_func_math = {\n",
    "        agr_colum: [agr_func]\n",
    "    }\n",
    "    dict_data=_data.groupby(grop_list, group_keys=True, dropna=True).agg(agg_func_math).to_dict()[(agr_colum, agr_func)]\n",
    "    df=pd.DataFrame(list(dict_data. items ()), columns = [grop_list_, agr_colum])\n",
    "    #df.set_index(df[grop_list], *, drop=True, append=False, inplace=False, verify_integrity=False)[source]\n",
    "    df.set_index(grop_list, inplace=True)\n",
    "    return df.copy()\n",
    "\n",
    "def concat_train_data(train_data):\n",
    "    Trap=group_data(train_data, 'Trap', 'Latitude', 'min')\n",
    "    Longitude=group_data(train_data, 'Trap', 'Longitude', 'min')\n",
    "    NumMosquitos=group_data(train_data, 'Trap', 'NumMosquitos', 'sum')\n",
    "    WnvPresent=group_data(train_data, 'Trap', 'WnvPresent', 'sum')\n",
    "    AddressAccuracy=group_data(train_data, 'Trap', 'AddressAccuracy', 'min')\n",
    "    RESTUANS=group_data(train_data, 'Trap', 'RESTUANS', 'sum')\n",
    "    PIPIENS=group_data(train_data, 'Trap', 'PIPIENS', 'sum')\n",
    "    Date_Min=group_data(train_data, 'Trap', 'Date', 'min')\n",
    "    #Date_Max=group_data(train_data, 'Trap', 'Date', 'max')\n",
    "    \n",
    "    frames = [Trap, Longitude, Date_Min, NumMosquitos, WnvPresent, AddressAccuracy, RESTUANS, PIPIENS]\n",
    "    result = pd.concat(frames, axis=1)\n",
    "    result.reset_index(inplace=True)\n",
    "    return result.copy()\n",
    "#train_data\n",
    "#df=concat_train_data(train_data[(train_data['year']==2013)])\n",
    "#df['test']='2021'\n",
    "#df\n",
    "#weather_data\n",
    "def make_category(X_Data, list_category):\n",
    "    for i in list_category:\n",
    "        X_Data['Cat_category']=X_Data[i].astype(\"category\")\n",
    "        clear_data(X_Data, [i], revers=False)\n",
    "        print(i)\n",
    "        X_Data.rename(\n",
    "            columns={'Cat_category': i,  # колонка \"SibSp\" буде перейменована на \"SiblingsSpouses\"\n",
    "                    }, inplace=True  # покажчик того, що зміни вносяться до існуючої таблиці\n",
    "        )\n",
    "    return X_Data\n",
    "\n",
    "def clear_data(test_data, drop_list, revers):\n",
    "    columns_df=[column for column in test_data]\n",
    "    #print(columns_df)\n",
    "    if revers==False:\n",
    "        for i in drop_list:\n",
    "            if i in columns_df:\n",
    "                test_data.drop(\n",
    "                    i,  # список колонок або рядків, які потрібно видалити\n",
    "                    axis=1,  # видалення рядків відбувається аналогічно, щоб видалити саме колонки, вибираємо відповідну вісь\n",
    "                    inplace=True  # видалення \"на місці\", без надання нової змінної\n",
    "                )\n",
    "    \n",
    "        \n",
    "    if revers==True:\n",
    "        for i in columns_df:\n",
    "            if i in drop_list:\n",
    "                pass\n",
    "                #print(i)\n",
    "            else:\n",
    "                #print(i)\n",
    "                test_data.drop(\n",
    "                    i,  # список колонок або рядків, які потрібно видалити\n",
    "                    axis=1,  # видалення рядків відбувається аналогічно, щоб видалити саме колонки, вибираємо відповідну вісь\n",
    "                    inplace=True  # видалення \"на місці\", без надання нової змінної)\n",
    "                )\n",
    "                \n",
    "            \n",
    "    return test_data.copy()\n",
    "\n",
    "def replase_M_to_Max_int(weather_data, replase_column, date_type):\n",
    "    #df=weather_data.copy()\n",
    "    try:\n",
    "        weather_data[replase_column].unique()\n",
    "        weather_data[replase_column]=weather_data[replase_column].replace(regex=['M'], value='-1000').astype(str).astype(date_type)\n",
    "        \n",
    "        M=weather_data[replase_column].max()\n",
    "        \n",
    "        weather_data.loc[(weather_data[replase_column] ==-1000), replase_column] = M\n",
    "        print(f'{replase_column}_{M} {weather_data[replase_column].unique()}')\n",
    "    except Exception as _ex:\n",
    "        print(f\"[{replase_column}] Error\", _ex)\n",
    "    return weather_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_data\n",
    "\n",
    "weather_data=clear_data(weather_data, ['Water1'], revers=False)\n",
    "list_category=['CodeSum']\n",
    "weather_data=make_category (weather_data, list_category)    \n",
    "\n",
    "#Depth_2\n",
    "replase_column='Depth'\n",
    "weather_data[replase_column].unique()\n",
    "level_map = {'0': 0, 'M': 1}\n",
    "weather_data[replase_column] = weather_data[replase_column].map(level_map).astype(str).astype(int)\n",
    "#weather_data[replase_column].unique()\n",
    "\n",
    "\n",
    "#SnowFall\n",
    "replase_column='SnowFall'\n",
    "level_map = {'0.0': 0, 'M': 0.5, '  T':1,  '0.1':0.1}\n",
    "weather_data[replase_column] = weather_data[replase_column].map(level_map).astype(str).astype(float)\n",
    "weather_data[replase_column].unique()\n",
    "\n",
    "#Sunrise\n",
    "replase_column='Sunrise'\n",
    "weather_data[replase_column] =weather_data[replase_column] .replace(regex=['-'],value='0').astype(str).astype(int)\n",
    "#weather_data[replase_column].unique()\n",
    "\n",
    "#Sunset\n",
    "replase_column='Sunset'\n",
    "weather_data[replase_column] =weather_data[replase_column] .replace(regex=['-'],value='0').astype(str).astype(int)\n",
    "#weather_data[replase_column].unique()\n",
    "\n",
    "#PrecipTotal\n",
    "replase_column='PrecipTotal'\n",
    "weather_data[replase_column] =weather_data[replase_column] .replace(regex=['M'],value='-1000')\n",
    "M=weather_data[replase_column].max()\n",
    "weather_data.loc[(weather_data[replase_column] =='-1000'), replase_column] = M\n",
    "print (M)\n",
    "\n",
    "\n",
    "\n",
    "weather_data[replase_column] =weather_data[replase_column] .replace(regex=['  T'],value='0')\n",
    "'''T=weather_data[replase_column].min()\n",
    "print (f'{T}_{weather_data.loc[(weather_data[replase_column] ==1000), replase_column]}')\n",
    "weather_data.loc[(weather_data[replase_column] ==1000), replase_column] = weather_data[replase_column].min()'''\n",
    "weather_data['1']=weather_data[replase_column].astype(str).astype(float)\n",
    "\n",
    "\n",
    "clear_data(weather_data, [replase_column], revers=False)\n",
    "\n",
    "weather_data.rename(\n",
    "            columns={'1': replase_column,  # колонка \"SibSp\" буде перейменована на \"SiblingsSpouses\"\n",
    "                    }, inplace=True  # покажчик того, що зміни вносяться до існуючої таблиці\n",
    "        )\n",
    "\n",
    "#weather_data[replase_column].unique()\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'Heat', int)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'Cool', int)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'Depart', int)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'Tavg', int)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'WetBulb', int)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'StnPressure', float)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'AvgSpeed', float)\n",
    "weather_data=replase_M_to_Max_int(weather_data, 'SeaLevel', float)\n",
    "weather_data=add_datetime(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def iter_days(draft_year, draft_month):\n",
    "    dt = datetime(draft_year, draft_month, 1)\n",
    "    while dt.year == draft_year and  dt.month == draft_month:\n",
    "        yield dt\n",
    "        dt += timedelta(days=1)\n",
    "\n",
    "\n",
    "train_data=add_datetime(train_data)\n",
    "train_data['RESTUANS']=train_data['Species'].str.contains('True').astype(str).astype(bool)\n",
    "train_data['PIPIENS']=train_data['Species'].str.contains('True').astype(str).astype(bool)\n",
    "     \n",
    "#draft_year_list={2007, 2009, 2013}\n",
    "#draft_month_list={6, 7, 8, 9}\n",
    "draft_year_list={2007, 2009, 2013}\n",
    "draft_month_list={6, 7, 8, 9}\n",
    "frames=[]\n",
    "for draft_year in draft_year_list:\n",
    "    data_year=train_data[train_data['year']==draft_year].copy()\n",
    "    #print(draft_year)\n",
    "    for draft_month in draft_month_list:\n",
    "        #print(draft_month)\n",
    "        train_data_month=data_year[(data_year['month']==draft_month)]\n",
    "        for dt in iter_days(draft_year, draft_month):\n",
    "            train_data_day=concat_train_data(train_data_month[(train_data_month['day']==dt.day)])\n",
    "            if train_data_day.shape[0]>0:\n",
    "                train_data_day.set_index('Date', inplace=False)\n",
    "                frames.append(train_data_day)\n",
    "\n",
    "result = pd.concat(frames, axis=0)\n",
    "result.reset_index()\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_truncated_data\n",
    "def concat_truncated_data(train_data):\n",
    "    Trap=group_data(train_data, 'Trap', 'Latitude', 'min')\n",
    "    Longitude=group_data(train_data, 'Trap', 'Longitude', 'min')\n",
    "    #NumMosquitos=group_data(train_data, 'Trap', 'NumMosquitos', 'sum')\n",
    "    #WnvPresent=group_data(train_data, 'Trap', 'WnvPresent', 'sum')\n",
    "    AddressAccuracy=group_data(train_data, 'Trap', 'AddressAccuracy', 'min')\n",
    "    RESTUANS=group_data(train_data, 'Trap', 'RESTUANS', 'sum')\n",
    "    PIPIENS=group_data(train_data, 'Trap', 'PIPIENS', 'sum')\n",
    "    Date_Min=group_data(train_data, 'Trap', 'Date', 'min')\n",
    "    #Date_Max=group_data(train_data, 'Trap', 'Date', 'max')\n",
    "    \n",
    "    frames = [Trap, Longitude, Date_Min, AddressAccuracy, RESTUANS, PIPIENS]\n",
    "    result = pd.concat(frames, axis=1)\n",
    "    result.reset_index(inplace=True)\n",
    "    return result.copy()\n",
    "\n",
    "test_truncated_data=add_datetime(test_truncated_data)\n",
    "test_truncated_data['RESTUANS']=test_truncated_data['Species'].str.contains('True').astype(str).astype(bool)\n",
    "test_truncated_data['PIPIENS']=test_truncated_data['Species'].str.contains('True').astype(str).astype(bool)\n",
    "test_truncated_data['Trap']=np.power(test_truncated_data['Latitude'], 2)+np.power(test_truncated_data['Longitude'], 2)\n",
    "#draft_year_list={2007, 2009, 2013}\n",
    "#draft_month_list={6, 7, 8, 9}\n",
    "\n",
    "draft_year_list={2011}\n",
    "draft_month_list={6, 7, 8, 9}\n",
    "frames=[]\n",
    "for draft_year in draft_year_list:\n",
    "    data_year=test_truncated_data[test_truncated_data['year']==draft_year].copy()\n",
    "    #print(draft_year)\n",
    "    for draft_month in draft_month_list:\n",
    "        #print(draft_month)\n",
    "        train_data_month=data_year[(data_year['month']==draft_month)]\n",
    "        for dt in iter_days(draft_year, draft_month):\n",
    "            test_truncated_data_day=concat_truncated_data(train_data_month[(train_data_month['day']==dt.day)])\n",
    "            if test_truncated_data_day.shape[0]>0:\n",
    "                test_truncated_data_day.set_index('Date', inplace=False)\n",
    "                frames.append(test_truncated_data_day)\n",
    "\n",
    "test_truncated_data_result = pd.concat(frames, axis=0)\n",
    "test_truncated_data_result.reset_index()\n",
    "test_truncated_data_result['Date']=np.datetime_as_string(test_truncated_data_result['Date'], unit='D')\n",
    "#.astype(object)\n",
    "test_truncated_data_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge weather_data and test_data\n",
    "\n",
    "weather_1=weather_data[weather_data['Station']==1].copy()\n",
    "weather_1.set_index('Date', inplace=True)\n",
    "weather_2=weather_data[weather_data['Station']==2].copy()\n",
    "weather_2.set_index('Date', inplace=True)\n",
    "\n",
    "weather=weather_1.merge(weather_2,\n",
    "    #weather_1, left_on='Date', right_on='Date')\n",
    "    how='inner',\n",
    "    #on=None, \n",
    "    left_on='Date', right_on='Date', \n",
    "    #left_index=True, right_index=True,\n",
    "    #, sort=False,\n",
    "    suffixes=('_1', '_2'), copy=True\n",
    "    #, indicator=False, validate=None\n",
    ")\n",
    "test_data=result.merge(weather,\n",
    "    #weather_1, left_on='Date', right_on='Date')\n",
    "    how='inner',\n",
    "    #on=None, \n",
    "    left_on='Date', right_on='Date', \n",
    "    #left_index=True, right_index=True,\n",
    "    #, sort=False,\n",
    "    #suffixes=('_x', '_y'), copy=True\n",
    "    #, indicator=False, validate=None\n",
    ")\n",
    "\n",
    "test_truncated_data=test_truncated_data_result.merge(weather,\n",
    "    #weather_1, left_on='Date', right_on='Date')\n",
    "    how='inner',\n",
    "    #on=None, \n",
    "    left_on='Date', right_on='Date', \n",
    "    #left_index=True, right_index=True,\n",
    "    #, sort=False,\n",
    "    #suffixes=('_x', '_y'), copy=True\n",
    "    #, indicator=False, validate=None\n",
    ")\n",
    "\n",
    "#test_truncated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data clear\n",
    "def clear_dtype(weather_data, drop_dtype_list):\n",
    "    for col in weather_data:\n",
    "        if str(weather_data[col].dtypes) in drop_dtype_list:\n",
    "            try:\n",
    "                weather_data.drop(\n",
    "                    weather_data[col].name,  # список колонок або рядків, які потрібно видалити\n",
    "                    axis=1,  # видалення рядків відбувається аналогічно, щоб видалити саме колонки, вибираємо відповідну вісь\n",
    "                    inplace=True  # видалення \"на місці\", без надання нової змінної\n",
    "                )\n",
    "                #X_Data=clear_data(X_Data, X_Data[col].name, revers=False)\n",
    "                print(f'{col}_{weather_data[col].name}_{weather_data[col].dtypes}')\n",
    "                return weather_data.copy()\n",
    "            except Exception as _ex:\n",
    "                print(f\"Error\", _ex)\n",
    "                \n",
    "                \n",
    "drop_list=['year_2', 'month_2', 'weekday_2', 'day_2']\n",
    "test_data=clear_data(test_data, drop_list, revers=False)\n",
    "test_truncated_data=clear_data(test_truncated_data, drop_list, revers=False)\n",
    "#drop_dtype_list=['bool', 'category', 'int64' ,'int32', 'float64', ]\n",
    "drop_dtype_list=['category']                \n",
    "clear_dtype(test_data, drop_dtype_list)\n",
    "clear_dtype(test_truncated_data, drop_dtype_list)\n",
    "test_truncated_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_drop_list=['NumMosquitos', 'WnvPresent', 'RESTUANS', 'PIPIENS', 'Trap', 'Date', 'Latitude', 'Longitude']\n",
    "\n",
    "X_drop_list=['NumMosquitos', 'WnvPresent', 'Trap', 'Date']\n",
    "X_Data=clear_data(test_data.copy(), X_drop_list, revers=False)\n",
    "\n",
    "Y_Data=clear_data(test_data.copy(), X_drop_list, revers=True)\n",
    "Y_Data=clear_data(Y_Data, ['Trap', 'Date'], revers=False)\n",
    "\n",
    "X_test_truncated_data=clear_data(test_truncated_data.copy(), X_drop_list, revers=False)\n",
    "\n",
    "\n",
    "X_drop_list=['Latitude', 'Longitude', 'month_1', 'day_1']\n",
    "#X_Data=clear_data(test_data.copy(), X_drop_list, revers=True)\n",
    "#X_test_truncated_data=clear_data(test_data.copy(), X_drop_list, revers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_Data,  # перший масив, який потрібно розділити\n",
    "    Y_Data,  # другий масив, який потрібно роздолити\n",
    "    test_size=0.2,  # розмір меншої групи (20% від загальної кількості даних)\n",
    "    random_state=42  # фіксація випадкового стану: тепер при перезапуску групи будуть однаковими\n",
    ")\n",
    "\n",
    "    # формальність для того, щоб не бачити попередження про зміни на зрізі з таблиці\n",
    "X_train, X_test = X_train.copy(), X_test.copy()\n",
    "    # побудуємо модель\n",
    "mdl = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    # оцінимо точність моделі\n",
    "train_predictions = mdl.predict(X_train)\n",
    "test_predictions = mdl.predict(X_test)\n",
    "truncated_predictions = mdl.predict(X_test_truncated_data)\n",
    "\n",
    "\n",
    "print('Помилка на навчальному наборі: ', mean_squared_error(y_train, train_predictions))\n",
    "print('Помилка на тестовому наборі: ', mean_squared_error(y_test, test_predictions))\n",
    "score = mdl.score(X_test, y_test)\n",
    "print(score)\n",
    "#print(mdl.score(X_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\functional.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# фіксація випадкового стану\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()  # ініціалізація порожньої моделі\n",
    "\n",
    "# додамо один перцептрон у модель\n",
    "model.add(Dense(\n",
    "    units=1,  # кількість перцептронів\n",
    "    activation='sigmoid'  # функція активації: перетворення на виході моделі\n",
    "))\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),  # алгоритм оптимізації та швидкість навчання\n",
    "    loss='binary_crossentropy',  # функція втрат (та сама, що в логістичної регресії)\n",
    "    metrics=['accuracy']  # додаткова метрика якості моделі\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = set(df.columns) - set(categorical_features)\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = df.copy()\n",
    "df_norm[list(continuous_features)] = scaler.fit_transform(df[list(continuous_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# будуємо модель у вигляді дерева\n",
    "mdl = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "# оцінимо якість моделі\n",
    "predictions = mdl.predict_proba(X_test)\n",
    "#auc = roc_auc_score(y_test, predictions, average='macro', multi_class='ovr')\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, coef_value in zip(X_train.columns, mdl.coef_):\n",
    "    print(var_name)\n",
    "    \n",
    "print(f'constant term: {var_name}', mdl.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centr_map():\n",
    "    map_ = folium.Map(location=[train_data['Latitude'].mean(),  train_data['Longitude'].mean()],\n",
    "               tiles=\"Stamen Terrain\", zoom_start=10, control_scale=False)\n",
    "\n",
    "    Station=folium.FeatureGroup(name='Station')\n",
    "    Station.add_to(map_)\n",
    "    Station.add_child(\n",
    "        folium.Marker([41.995,  -87.933],\n",
    "                      popup=\"CHICAGO O'HARE INTERNATIONAL AIRPORT\").add_to(map_))\n",
    "    Station.add_child(\n",
    "        folium.Marker(\n",
    "            [41.786,  -88.089163], popup=\"CHICAGO MIDWAY INTL ARPT\").add_to(map_))\n",
    "    return map_ \n",
    "\n",
    "map_=centr_map()\n",
    "\n",
    "\n",
    "\n",
    "df=spray_data\n",
    "time_index = list(spray_data['Date'].sort_values().astype('str').unique())\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].sort_values(ascending=True)\n",
    "data = []\n",
    "\n",
    "for _, d in df.groupby('Date'):\n",
    "    #print(f'{row}')\n",
    "    data.append([[row['Latitude'], row['Longitude'], 1] for _, row in d.iterrows()])\n",
    "\n",
    "hm = plugins.HeatMapWithTime(\n",
    "    data,\n",
    "    name=\"Spray\",\n",
    "    index=time_index,\n",
    "    auto_play=True,\n",
    "    min_opacity=0,\n",
    "    max_opacity=0.7,\n",
    "    #gradient={0.1:'blue', 0.25:'green', 0.5:'yelow', 0.75:'orange', 1:'red'} \n",
    ")\n",
    "hm.add_to(map_)\n",
    "\n",
    "folium.LayerControl().add_to(map_)\n",
    "map_.save(f\"sklearn_linear_model.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_=centr_map()\n",
    "\n",
    "\n",
    "\n",
    "df=test_data.copy()\n",
    "time_index = list(test_data['Date'].sort_values().astype('str').unique())\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].sort_values(ascending=True)\n",
    "data = []\n",
    "\n",
    "for _, d in df.groupby('Date'):\n",
    "    #print(f'{row}')\n",
    "    data.append([[row['Latitude'], row['Longitude'], row['NumMosquitos']] for _, row in d.iterrows()])\n",
    "\n",
    "test_data_hm = plugins.HeatMapWithTime(\n",
    "    data,\n",
    "    name=\"test_data\",\n",
    "    index=time_index,\n",
    "    auto_play=True,\n",
    "    min_opacity=0,\n",
    "    max_opacity=0.7,\n",
    "    #gradient={0.1:'blue', 0.25:'green', 0.5:'yelow', 0.75:'orange', 1:'red'} \n",
    ")\n",
    "test_data_hm.add_to(map_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(map_)\n",
    "map_.save(f\"test_data.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_markers(): \n",
    "    # train_data_point_map\n",
    "    draft_year_list={2007, 2009, 2013}\n",
    "    draft_month_list={6, 7, 8, 9}\n",
    "    geometry = geopandas.points_from_xy(train_data.Longitude, train_data.Latitude)\n",
    "    geo_df = geopandas.GeoDataFrame(\n",
    "        train_data, geometry=geometry\n",
    "    )\n",
    "    geo_df_list = [[point.xy[1][0], point.xy[0][0]] for point in geo_df.geometry]\n",
    "\n",
    "    wnvpresentsLayer=folium.FeatureGroup(name='wnvpresents')\n",
    "    wnvpresentsLayer.add_to(map_)\n",
    "\n",
    "    # Iterate through list and add a marker for each volcano, color-coded by its type.\n",
    "    i = 0\n",
    "    for coordinates in geo_df_list:\n",
    "        if (geo_df.month[i] == draft_month) and  (geo_df.year[i] == draft_year):\n",
    "            wnvpresents=int(train_data[(geo_df['Trap'] == geo_df.Trap[i]) & (geo_df['month']==draft_month) \n",
    "                       & (geo_df['year']==draft_year)]['WnvPresent'].sum())\n",
    "            if wnvpresents>0:\n",
    "                if geo_df.PIPIENS[i] == True:\n",
    "                    type_color = \"green\"\n",
    "                if (geo_df.RESTUANS[i] == True):\n",
    "                    type_color = \"red\"\n",
    "                    # Place the markers with the popup labels and data\n",
    "                wnvpresentsLayer.add_child(\n",
    "                    folium.Marker(\n",
    "                        location=coordinates,\n",
    "                        popup=geo_df.Trap[i]+ \"<br>\"\n",
    "                        +'wnvpresents '+ str(wnvpresents)+ \"<br>\"+'numMosquitos '+ str(numMosquitos(draft_year, draft_month)),\n",
    "                        icon=folium.Icon(color=\"%s\" % type_color)\n",
    "                        )\n",
    "                )\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datasetes():\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plot_mas = test_truncated_data.groupby(['year'], group_keys=True,\n",
    "                                           dropna=True).agg({'year': ['count']}).to_dict()[('year', 'count')] \n",
    "    plt.bar(plot_mas.keys(), plot_mas.values(), color='blue', label='test_truncated_data counts  in year')\n",
    "    plt.legend()\n",
    "    plt.grid(True) \n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plot_mas = weather_data.groupby(['year'], group_keys=True,\n",
    "                                           dropna=True).agg({'year': ['count']}).to_dict()[('year', 'count')] \n",
    "    plt.bar(plot_mas.keys(), plot_mas.values(), color='pink', label='weather_data counts  in year')\n",
    "    plt.legend()\n",
    "    plt.grid(True)   # лінії допоміжної сітки\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    #fig2 = plt.figure()\n",
    "\n",
    "    plot_mas = train_data.groupby(['year'], group_keys=True,\n",
    "                                           dropna=True).agg({'year': ['count']}).to_dict()[('year', 'count')] \n",
    "    plt.bar(plot_mas.keys(), plot_mas.values(), color='green', label='train_data counts  in year')\n",
    "    plt.legend()\n",
    "    plt.grid(True)   # лінії допоміжної сітки\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plot_mas = spray_data.groupby(['year'], group_keys=True,\n",
    "                                           dropna=True).agg({'year': ['count']}).to_dict()[('year', 'count')] \n",
    "    plt.bar(plot_mas.keys(), plot_mas.values(), color='red', label='spray_data counts  in year')\n",
    "    plt.legend()\n",
    "    plt.grid(True)   # лінії допоміжної сітки\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plot_mas = test_truncated_data.groupby(['year'], group_keys=True,\n",
    "                                           dropna=True).agg({'year': ['count']}).to_dict()[('year', 'count')] \n",
    "    plt.bar(plot_mas.keys(), plot_mas.values(), color='red', label='test_truncated_data counts  in year')\n",
    "    plt.legend()\n",
    "    plt.grid(True)   # лінії допоміжної сітки\n",
    "\n",
    "plot_datasetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chat(_data, grop_list_, agr_colum, agr_func):\n",
    "    grop_list=[grop_list_]\n",
    "    agg_func_math = {\n",
    "        agr_colum: [agr_func]\n",
    "    }\n",
    "\n",
    "    plot_mas = _data.groupby(grop_list, group_keys=True, dropna=True).agg(agg_func_math).to_dict()[(agr_colum, agr_func)] \n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.bar(plot_mas.keys(), plot_mas.values())\n",
    "    plt.title(f'{grop_list_} {agr_colum} {agr_func} chart')\n",
    "    \n",
    "    plt.grid(True)   # лінії допоміжної сітки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_mas = train_data.groupby(['month'], group_keys=True,\n",
    "                              dropna=True).agg({'WnvPresent': ['sum']}).to_dict()[('WnvPresent', 'sum')] \n",
    "plt.bar(plot_mas.keys(), plot_mas.values(), color='red', label='WnvPresent sum month')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(False)   # лінії допоміжної сітки\n",
    "#Date\n",
    "fig = plt.figure()\n",
    "plot_mas = train_data.groupby(['Date'], group_keys=True,\n",
    "                              dropna=True).agg({'WnvPresent': ['sum']}).to_dict()[('WnvPresent', 'sum')] \n",
    "plt.bar(plot_mas.keys(), plot_mas.values(), color='red', label='WnvPresent sum Date')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(False)   # лінії допоміжної сітки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_adres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our key point when map open it\n",
    "map_ = folium.Map(location=[train_data_adres['Latitude'].mean(),  train_data_adres['Longitude'].mean()],\n",
    "               tiles=\"Stamen Terrain\", zoom_start=10, control_scale=False)\n",
    "\n",
    "for la,lo in train_data_adres:\n",
    "    folium.Marker(\n",
    "        location=[la,lo],\n",
    "        icon=folium.Icon(icon_color='white')\n",
    "    ).add_to(map2)\n",
    "# Plotting \n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ = folium.Map(location=[train_data_adres['Latitude'].mean(),  train_data_adres['Longitude'].mean()],\n",
    "               tiles=\"Stamen Terrain\", zoom_start=10, control_scale=False)\n",
    "\n",
    "folium.Marker(train_data_adres['Latitude','Longitude'], popup=train_data_adres['Trap']).add_to(map_)\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import folium\n",
    "#Station_1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\n",
    "#Station_2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level\n",
    "\n",
    "#m = folium.Map(location=[45.5236, -122.6750])\n",
    "#m = folium.Map(location=[42.391623, -88.089163], tiles=\"Stamen Terrain\", zoom_start=13)\n",
    "\n",
    "\n",
    "map_ = folium.Map(location=[spray_data['Latitude'].mean(),  spray_data['Longitude'].mean()],\n",
    "               tiles=\"Stamen Terrain\", zoom_start=10, control_scale=False)\n",
    "\n",
    "folium.Marker([41.995,  -87.933], popup=\"CHICAGO O'HARE INTERNATIONAL AIRPORT\").add_to(map_)\n",
    "folium.Marker([41.786,  -88.089163], popup=\"CHICAGO MIDWAY INTL ARPT\").add_to(map_)\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[spray_data['Latitude'].mean(),  spray_data['Longitude'].mean()],\n",
    "    radius=10,\n",
    "    popup=\"spray_mean\",\n",
    "    color=\"#3186cc\",\n",
    "    fill=True,\n",
    "    fill_color=\"#3186cc\",\n",
    ").add_to(map_)\n",
    "\n",
    "\n",
    "#train_data_adres\n",
    "#for index, location_info in train_data.iterrows():\n",
    "#    folium.Marker([location_info[\"Latitude\"], location_info[\"Longitude\"]], popup=location_info[\"NumMosquitos\"]).add_to(map_)\n",
    "\n",
    "map_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
